{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries ","metadata":{}},{"cell_type":"code","source":"import csv  # for reading and writing CSV files\nimport pandas as pd  # for data manipulation and analysis\nimport numpy as np  # for numerical computing\nimport math, random  # for mathematical operations and random number generation\nimport matplotlib.pyplot as plt # for plots\nimport seaborn as sns # for plots\n\nfrom pathlib import Path  # for working with file paths and directories\nimport glob  # for finding files and directories based on wildcard patterns\n\nimport librosa  # for audio analysis and processing\nfrom IPython.display import Audio  # for playing audio\nimport torchaudio  # for audio processing\nfrom torchaudio import transforms\n\nfrom sklearn import preprocessing  # for data preprocessing\n\nimport torch  # for deep learning tasks\nimport io  # for working with input and output streams\nimport torch.nn as nn  # for neural network-related classes and functions\nfrom torch.nn import init  # for weight initialization\nfrom torch.utils.data import DataLoader, Dataset, random_split  # for data loading and dataset management\nimport torch.nn.functional as F  # for ????\n\nimport gc # garbage collector to automatically reclaim memory occupied by objects that are no longer in use","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-24T13:21:48.294759Z","iopub.execute_input":"2023-06-24T13:21:48.295218Z","iopub.status.idle":"2023-06-24T13:21:48.304068Z","shell.execute_reply.started":"2023-06-24T13:21:48.295179Z","shell.execute_reply":"2023-06-24T13:21:48.302905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the Data","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Analysis of Metadata","metadata":{}},{"cell_type":"code","source":"# Read the CSV file \"train_metadata.csv\" and assign the data to the variable df\ndf = pd.read_csv(\"/kaggle/input/birdclef-2023/train_metadata.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:48.305826Z","iopub.execute_input":"2023-06-24T13:21:48.306742Z","iopub.status.idle":"2023-06-24T13:21:48.436789Z","shell.execute_reply.started":"2023-06-24T13:21:48.306710Z","shell.execute_reply":"2023-06-24T13:21:48.435844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:48.439025Z","iopub.execute_input":"2023-06-24T13:21:48.439735Z","iopub.status.idle":"2023-06-24T13:21:48.471955Z","shell.execute_reply.started":"2023-06-24T13:21:48.439701Z","shell.execute_reply":"2023-06-24T13:21:48.470887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the first few rows of the DataFrame\nprint(\"DataFrame Preview:\")\nprint(df.head())\n\n# Check the dimensions of the DataFrame (number of rows, number of columns)\nprint(\"DataFrame Dimensions:\")\nprint(df.shape)\n\n# Check the column names and their data types\nprint(\"Column Names and Data Types:\")\nprint(df.dtypes)\n\n# Check for missing values in each column\nprint(\"Missing Values:\")\nprint(df.isnull().sum())\n\n# Check the unique values in the 'primary_label' column\nprint(\"Unique Labels:\")\nprint(df['primary_label'].unique())\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:48.474909Z","iopub.execute_input":"2023-06-24T13:21:48.475355Z","iopub.status.idle":"2023-06-24T13:21:48.541468Z","shell.execute_reply.started":"2023-06-24T13:21:48.475320Z","shell.execute_reply":"2023-06-24T13:21:48.540601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOTE: We have 227 missing values in latitude and longitude --> Need to clean","metadata":{}},{"cell_type":"code","source":"# Statistical Summary\ndf_summary =df.describe()\nprint(df_summary)","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:48.542487Z","iopub.execute_input":"2023-06-24T13:21:48.542815Z","iopub.status.idle":"2023-06-24T13:21:48.563010Z","shell.execute_reply.started":"2023-06-24T13:21:48.542782Z","shell.execute_reply":"2023-06-24T13:21:48.562114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8))  # Increase the figure size for better visibility\n\nax = sns.countplot(x='primary_label', data=df)\nax.set_xlabel('Bird Species')\nax.set_ylabel('Count')\nax.set_title('Distribution of Bird Species')\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)  # Adjust the rotation angle to prevent overlapping\n\n# Move the x-axis label to the side\nax.xaxis.set_label_coords(0.5, -0.1)\n\n# Move the y-axis label to the side\nax.yaxis.set_label_coords(-0.1, 0.5)\n\nplt.tight_layout()  # Ensure all elements fit within the plot area\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:48.564473Z","iopub.execute_input":"2023-06-24T13:21:48.564902Z","iopub.status.idle":"2023-06-24T13:21:51.505928Z","shell.execute_reply.started":"2023-06-24T13:21:48.564872Z","shell.execute_reply":"2023-06-24T13:21:51.505045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing Numeric Variables\nplt.figure(figsize=(10, 6))\nsns.histplot(x='latitude', data=df, bins=20)\nplt.xlabel('Latitude')\nplt.ylabel('Count')\nplt.title('Distribution of Latitude')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:51.506921Z","iopub.execute_input":"2023-06-24T13:21:51.507248Z","iopub.status.idle":"2023-06-24T13:21:51.851240Z","shell.execute_reply.started":"2023-06-24T13:21:51.507218Z","shell.execute_reply":"2023-06-24T13:21:51.850322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring Relationships\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='latitude', y='longitude', data=df, hue='primary_label', legend=False)\nplt.xlabel('Latitude')\nplt.ylabel('Longitude')\nplt.title('Geographical Distribution of Bird Species')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:51.852502Z","iopub.execute_input":"2023-06-24T13:21:51.852933Z","iopub.status.idle":"2023-06-24T13:21:52.578438Z","shell.execute_reply.started":"2023-06-24T13:21:51.852899Z","shell.execute_reply":"2023-06-24T13:21:52.577446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Geospatial Analysis\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Create a GeoDataFrame with latitude and longitude\ngeometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\ngdf = gpd.GeoDataFrame(df, geometry=geometry)\n\n# Plot the recording locations on a map\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nfig, ax = plt.subplots(figsize=(10, 10))\nworld.plot(ax=ax, color='lightgray')\ngdf.plot(ax=ax, markersize=5, color='red', alpha=0.7)\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Recording Locations')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:52.580114Z","iopub.execute_input":"2023-06-24T13:21:52.580458Z","iopub.status.idle":"2023-06-24T13:21:56.114031Z","shell.execute_reply.started":"2023-06-24T13:21:52.580430Z","shell.execute_reply":"2023-06-24T13:21:56.113112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Peprocess the data by creating a new column for the relative path of audio files, selecting relevant columns, encoding the class labels, and organizing the DataFrame for model training.\n# Create a new column 'relative_path' by concatenating 'train_audio/' with 'filename'\ndf['relative_path'] = 'train_audio/' + df['filename'].astype(str)\n\n# Select only the 'relative_path' and 'primary_label' columns\ndf = df[['relative_path', 'primary_label']]\n\n# Get the unique classes from the 'primary_label' column and sort them\nclasses = sorted(df.primary_label.unique())\n\n# Encode the 'primary_label' column to numerical class labels using LabelEncoder\nle = preprocessing.LabelEncoder()\ndf[['classID']] = df[['primary_label']].apply(le.fit_transform)\n\n# Delete the 'primary_label' column from the DataFrame\ndel df['primary_label']","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:56.118541Z","iopub.execute_input":"2023-06-24T13:21:56.119133Z","iopub.status.idle":"2023-06-24T13:21:56.144939Z","shell.execute_reply.started":"2023-06-24T13:21:56.119097Z","shell.execute_reply":"2023-06-24T13:21:56.144065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['classID']]","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:56.146317Z","iopub.execute_input":"2023-06-24T13:21:56.146688Z","iopub.status.idle":"2023-06-24T13:21:56.157555Z","shell.execute_reply.started":"2023-06-24T13:21:56.146655Z","shell.execute_reply":"2023-06-24T13:21:56.156293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning Data","metadata":{}},{"cell_type":"code","source":" df['relative_path']","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:56.159252Z","iopub.execute_input":"2023-06-24T13:21:56.159689Z","iopub.status.idle":"2023-06-24T13:21:56.171027Z","shell.execute_reply.started":"2023-06-24T13:21:56.159659Z","shell.execute_reply":"2023-06-24T13:21:56.169406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect and handle duplicate rows\nduplicate_rows = df.duplicated()\nif duplicate_rows.any():\n    # Display the duplicate rows\n    print(\"Duplicate rows:\")\n    print(df[duplicate_rows])\n    # Drop the duplicate rows\n    df = df.drop_duplicates()\nelse: print(\"-> There are no duplicate rows to drop.\")","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:56.172417Z","iopub.execute_input":"2023-06-24T13:21:56.172681Z","iopub.status.idle":"2023-06-24T13:21:56.184215Z","shell.execute_reply.started":"2023-06-24T13:21:56.172660Z","shell.execute_reply":"2023-06-24T13:21:56.183175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mapping the classes and counting the unsupported classes","metadata":{}},{"cell_type":"code","source":"# Create a list of classes that match the encoded class labels\ncompetition_classes = classes\n\n# Count the number of classes that are not supported by the pretrained model\nforced_defaults = 0\ncompetition_class_map = []\nfor c in competition_classes:\n    try:\n        i = classes.index(c)\n        competition_class_map.append(i)\n    except:\n        competition_class_map.append(0)\n        forced_defaults += 1\n\n# Print the count of classes not supported by the pretrained model\nforced_defaults","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:56.185698Z","iopub.execute_input":"2023-06-24T13:21:56.186062Z","iopub.status.idle":"2023-06-24T13:21:56.195969Z","shell.execute_reply.started":"2023-06-24T13:21:56.186030Z","shell.execute_reply":"2023-06-24T13:21:56.194872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess the data","metadata":{}},{"cell_type":"markdown","source":"We will build functions for all pre-processing steps, so later on the pro-processing is quickly done.","metadata":{}},{"cell_type":"code","source":"# Framing a given signal into smaller overlapping frames\nimport torch.nn.functional as F  \n\ndef frame(signal, frame_length, frame_step, pad_end=False, pad_value=1000, axis=-1):\n   \"\"\"\n    Frame a given signal into smaller overlapping frames.\n\n    Args:\n        signal (ndarray): The input signal to be framed.\n        frame_length (int): The length of each frame in samples.\n        frame_step (int): The number of samples to advance to the next frame.\n        pad_end (bool, optional): Whether to pad the end of the signal to ensure all frames have the same length. Defaults to False.\n        pad_value (int, optional): The value used for padding the signal. Defaults to 1000.\n        axis (int, optional): The axis along which to frame the signal. Defaults to -1.\n\n    Returns:\n        ndarray: The framed signal tensor representing the overlapping frames.\n\n    \"\"\"\n    signal_length = signal.shape[axis]  # Get the length of the 'signal' along the specified 'axis'\n    \n    if pad_end:\n        frames_overlap = frame_length - frame_step  # Calculate the number of overlapping samples between frames\n        rest_samples = np.abs(signal_length - frames_overlap) % np.abs(frame_length - frames_overlap)  # Calculate the remaining samples after framing\n        pad_size = int(frame_length - rest_samples)  # Calculate the size of padding needed\n        if pad_size != 0:  # If padding is required\n            pad_axis = [0] * signal.ndim  # Create a list for padding axes\n            pad_axis[axis] = pad_size  # Set the size of padding along the specified 'axis'\n            # Calculate the padding size needed for both ends\n            left_pad = pad_size // 2\n            right_pad = pad_size - left_pad\n            signal = F.pad(signal, (left_pad, right_pad), \"constant\", pad_value)  # Apply padding to the signal using 'F.pad' function\n    \n    frames = signal.unfold(axis, frame_length, frame_step)  # Unfold the 'signal' tensor to extract overlapping frames\n    \n    return frames  # Return the 'frames' tensor representing the framed signal\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:56.197716Z","iopub.execute_input":"2023-06-24T13:21:56.198183Z","iopub.status.idle":"2023-06-24T13:21:56.209377Z","shell.execute_reply.started":"2023-06-24T13:21:56.198150Z","shell.execute_reply":"2023-06-24T13:21:56.208508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# framing audio signals\ndef frame_audio(audio_array: np.ndarray, window_size_s: float = 5.0, hop_size_s: float = 5.0, sample_rate=32000) -> np.ndarray:\n    \"\"\"Helper function for framing audio signals for inference. Frames the input audio array using the specified window size, hop size, and sample rate.\"\"\"\n\n    if window_size_s is None or window_size_s < 0:\n        # If window_size_s is None or negative, return audio_array with an additional axis\n        return audio_array[np.newaxis, :]\n\n    frame_length = int(window_size_s * sample_rate)\n    hop_length = int(hop_size_s * sample_rate)\n    max_len = sample_rate // 1000 * window_size_s\n\n    framed_audio = frame(audio_array, frame_length, hop_length, pad_value=max_len, pad_end=True)\n    # Frame the audio_array using the specified frame_length, hop_length, and pad_value.\n    # The pad_end parameter is set to True to pad the end of the audio if necessary.\n    return framed_audio","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:56.210877Z","iopub.execute_input":"2023-06-24T13:21:56.211195Z","iopub.status.idle":"2023-06-24T13:21:56.219837Z","shell.execute_reply.started":"2023-06-24T13:21:56.211165Z","shell.execute_reply":"2023-06-24T13:21:56.218949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checks if the original_sample_rate matches the desired_sample_rate. \ndef ensure_sample_rate(waveform, original_sample_rate, desired_sample_rate=32000):\n    \"\"\"Resample waveform if required.\"\"\"\n    if original_sample_rate != desired_sample_rate:\n        # Convert the NumPy array to a PyTorch tensor with a floating-point data type\n        audio_tensor = torch.from_numpy(waveform).float()\n\n        # Resample the waveform using torchaudio.transforms.Resample\n        waveform = torchaudio.transforms.Resample(original_sample_rate, desired_sample_rate)(audio_tensor)\n\n    return desired_sample_rate, waveform\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:56.221342Z","iopub.execute_input":"2023-06-24T13:21:56.221779Z","iopub.status.idle":"2023-06-24T13:21:56.233707Z","shell.execute_reply.started":"2023-06-24T13:21:56.221750Z","shell.execute_reply":"2023-06-24T13:21:56.232913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Examples","metadata":{}},{"cell_type":"code","source":"import librosa\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\n\n# Load the audio file using librosa and obtain the audio data and sample rate\naudio0, sample_rate = librosa.load(\"/kaggle/input/birdclef-2023/train_audio/abethr1/XC128013.ogg\")\n\n# Ensure that the sample rate of the audio matches the desired sample rate\nsample_rate, wav_data = ensure_sample_rate(audio0, sample_rate)\n\n# Plot the waveform\nplt.figure(figsize=(10, 4))\nplt.plot(wav_data)\nplt.title('Waveform')\nplt.xlabel('Time')\nplt.ylabel('Amplitude')\nplt.show()\n\n# Display the audio data as an audio widget\nAudio(wav_data, rate=sample_rate)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:21:56.235368Z","iopub.execute_input":"2023-06-24T13:21:56.236120Z","iopub.status.idle":"2023-06-24T13:22:28.196332Z","shell.execute_reply.started":"2023-06-24T13:21:56.236081Z","shell.execute_reply":"2023-06-24T13:22:28.195131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the audio file using librosa and obtain the audio data and sample rate\naudio1, sample_rate = librosa.load(\"/kaggle/input/birdclef-2023/train_audio/abethr1/XC363501.ogg\")\n\n# Ensure that the sample rate of the audio matches the desired sample rate\nsample_rate, wav_data = ensure_sample_rate(audio1, sample_rate)\n\n# Plot the waveform\nplt.figure(figsize=(10, 4))\nplt.plot(wav_data)\nplt.title('Waveform')\nplt.xlabel('Time')\nplt.ylabel('Amplitude')\nplt.show()\n\n# Display the audio data as an audio widget\nAudio(wav_data, rate=sample_rate)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:22:28.198010Z","iopub.execute_input":"2023-06-24T13:22:28.199577Z","iopub.status.idle":"2023-06-24T13:22:37.966550Z","shell.execute_reply.started":"2023-06-24T13:22:28.199521Z","shell.execute_reply":"2023-06-24T13:22:37.965758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Model","metadata":{}},{"cell_type":"code","source":"class AudioUtil():\n     '''Utility class for audio processing operations'''\n  @staticmethod\n  def open(audio_file):\n    '''Loads the input audio file. Returns the signal as a tensor and the sample rate.'''\n    sig, sr = torchaudio.load(audio_file)\n    return (sig, sr)\n\n  # Convert the given audio to the desired number of channels\n  @staticmethod\n  def rechannel(aud, new_channel):\n        '''Adjust the given audio signal to have the given number of audio channels.'''\n    sig, sr = aud\n\n    if (sig.shape[0] == new_channel):\n      # Nothing to do\n      return aud\n\n    if (new_channel == 1):\n      # Convert from stereo to mono by selecting only the first channel\n      resig = sig[:1, :]\n    else:\n      # Convert from mono to stereo by duplicating the first channel\n      resig = torch.cat([sig, sig])\n\n    return ((resig, sr))\n\n  # Resample the audio to the desired sample rate\n  @staticmethod\n  def resample(aud, newsr):\n        '''Resamples the input audio, returns audio with the desired new sample rate'''\n    sig, sr = aud\n\n    if (sr == newsr):\n      # Nothing to do\n      return aud\n\n    num_channels = sig.shape[0]\n    # Resample the first channel\n    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1, :])\n    if (num_channels > 1):\n      # Resample the second channel and merge both channels\n      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:, :])\n      resig = torch.cat([resig, retwo])\n\n    return ((resig, newsr))\n\n\n  @staticmethod\n  def pad_trunc(aud, max_ms):\n    '''Pad or truncate the input audio signal to the given fixed length'''\n    sig, sr = aud\n    num_rows, sig_len = sig.shape\n    max_len = sr // 1000 * max_ms\n\n    if (sig_len > max_len):\n      # Truncate the signal to the given length\n      sig = sig[:, :max_len]\n\n    elif (sig_len < max_len):\n      # Length of padding to add at the beginning and end of the signal\n      pad_begin_len = random.randint(0, max_len - sig_len)\n      pad_end_len = max_len - sig_len - pad_begin_len\n\n      # Pad with 0s\n      pad_begin = torch.zeros((num_rows, pad_begin_len))\n      pad_end = torch.zeros((num_rows, pad_end_len))\n\n      sig = torch.cat((pad_begin, sig, pad_end), 1)\n      \n    return (sig, sr)\n\n  # Shifts the audio signal to the left or right by some percent\n  @staticmethod\n  def time_shift(aud, shift_limit):\n         '''\n        Applies a random time shift to an audio signal.\n\n        Args:\n            aud (tuple): the audio signal and its sample rate.\n            shift_limit (float): The maximum time shift limit as a fraction of the signal length.\n\n        Returns the time-shifted audio signal and its sample rate.\n\n        '''\n    sig, sr = aud\n    _, sig_len = sig.shape\n    shift_amt = int(random.random() * shift_limit * sig_len)\n    return (sig.roll(shift_amt), sr)\n\n  # Generate a spectrogram from the audio signal\n  @staticmethod\n  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n        '''\n        Computes and returns the spectrogram (np.ndarray) of an audio signal.\n\n        Args:\n            aud (tuple): containing the audio signal and its sample rate.\n            n_mels (int): The number of Mel frequency bins.\n            n_fft (int): The number of FFT points.\n            hop_len (int or None): The hop length in samples. If None, it is set to n_fft // 4.\n\n        '''\n    sig, sr = aud\n    top_db = 80\n\n    # Compute the Mel spectrogram\n    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n\n    # Convert to decibels\n    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n    return (spec)\n\n  # Augment the spectrogram by masking out sections to prevent overfitting\n  @staticmethod\n  def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n         '''\n        Augments the spectrogram to prevent overfitting by masking out sections. Returns the augmented spectrogram (np.ndarray).\n\n        Args:\n            spec (numpy.ndarray): The input spectrogram.\n            max_mask_pct (float): The maximum percentage of the spectrogram to be masked.\n            n_freq_masks (int): The number of frequency masks to apply.\n            n_time_masks (int): The number of time masks to apply.\n\n        '''\n    _, n_mels, n_steps = spec.shape\n    mask_value = spec.mean()\n    aug_spec = spec\n\n    freq_mask_param = max_mask_pct * n_mels\n    for _ in range(n_freq_masks):\n      aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n\n    time_mask_param = max_mask_pct * n_steps\n    for _ in range(n_time_masks):\n      aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n\n    return aug_spec","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:22:37.967898Z","iopub.execute_input":"2023-06-24T13:22:37.968448Z","iopub.status.idle":"2023-06-24T13:22:37.989429Z","shell.execute_reply.started":"2023-06-24T13:22:37.968416Z","shell.execute_reply":"2023-06-24T13:22:37.988453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build `SoundDS` class to handle audio data for training and inference\n- custom dataset class designed to handle audio data for training or inference purposes\n- inherits from the `Dataset` class (PyTorch class used for creating custom datasets)","metadata":{}},{"cell_type":"code","source":"class SoundDS(Dataset):\n    '''Custom dataset class for audio data. Input: df and path to data'''\n  def __init__(self, df, data_path):\n    self.df = df\n    self.data_path = str(data_path)\n    self.duration = 4000\n    self.sr = 44100\n    self.channel = 2\n    self.shift_pct = 0.4\n    \n  def __len__(self):\n    return len(self.df) \n\n  def __getitem__(self, idx):\n    audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n    class_id = self.df.loc[idx, 'classID']\n   \n    # Load the audio file and get the signal and sample rate\n    aud = AudioUtil.open(audio_file)\n    \n    # Resample the audio to the desired sample rate\n    reaud = AudioUtil.resample(aud, self.sr)\n    \n    # Convert the audio to the desired number of channels\n    rechan = AudioUtil.rechannel(reaud, self.channel)\n    \n    # Pad or truncate the audio signal to a fixed duration\n    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n    \n    # Shift the audio signal by a random percentage\n    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n    \n    # Generate a spectrogram from the shifted audio signal\n    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n    \n    # Augment the spectrogram by applying frequency and time masking\n    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n\n    return aug_sgram, class_id","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:22:37.990989Z","iopub.execute_input":"2023-06-24T13:22:37.991539Z","iopub.status.idle":"2023-06-24T13:22:38.009170Z","shell.execute_reply.started":"2023-06-24T13:22:37.991509Z","shell.execute_reply":"2023-06-24T13:22:38.008173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the model\nBuild the class `AudioClassifier` to define the model architecture and forward pass of the audio classifier\n\n- inherits from `nn.Module`","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\nclass AudioClassifier(nn.Module):\n    '''defines the architecture and forward pass of an audio classifier model in PyTorch'''\n    def __init__(self):\n        '''\n        Initializes an instance of the AudioClassifier class.\n\n        The constructor builds the model architecture by defining the convolutional blocks,\n        the adaptive average pooling layer, and the linear classifier. The weights of the\n        convolutional layers are initialized using the Kaiming initialization method.\n\n        '''\n        super().__init__()\n        conv_layers = []\n\n        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n        self.relu1 = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        init.kaiming_normal_(self.conv1.weight, a=0.1)\n        self.conv1.bias.data.zero_()\n        conv_layers += [self.conv1, self.relu1, self.bn1]\n\n        # Second Convolution Block\n        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu2 = nn.ReLU()\n        self.bn2 = nn.BatchNorm2d(16)\n        init.kaiming_normal_(self.conv2.weight, a=0.1)\n        self.conv2.bias.data.zero_()\n        conv_layers += [self.conv2, self.relu2, self.bn2]\n\n        # Third Convolution Block\n        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu3 = nn.ReLU()\n        self.bn3 = nn.BatchNorm2d(32)\n        init.kaiming_normal_(self.conv3.weight, a=0.1)\n        self.conv3.bias.data.zero_()\n        conv_layers += [self.conv3, self.relu3, self.bn3]\n\n        # Fourth Convolution Block\n        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.relu4 = nn.ReLU()\n        self.bn4 = nn.BatchNorm2d(64)\n        init.kaiming_normal_(self.conv4.weight, a=0.1)\n        self.conv4.bias.data.zero_()\n        conv_layers += [self.conv4, self.relu4, self.bn4]\n\n        # Linear Classifier\n        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n        self.lin = nn.Linear(in_features=64, out_features=264)\n\n        # Wrap the Convolutional Blocks\n        self.conv = nn.Sequential(*conv_layers)\n \n    # Forward pass computations\n    def forward(self, x):\n        '''\n        Performs the forward pass computations of the AudioClassifier.\n\n        Args:\n            x (torch.Tensor): spectrogram tensor with shape = (batch_size, num_channels, height, width).\n\n        Returns:\n            torch.Tensor: The output tensor representing the logits of the predicted\n                audio classes. Its shape is (batch_size, num_classes).\n        '''\n        # Run the convolutional blocks\n        x = self.conv(x)\n\n        # Adaptive pool and flatten for input to linear layer\n        x = self.ap(x)\n        x = x.view(x.shape[0], -1)\n\n        # Linear layer\n        x = self.lin(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:22:38.011026Z","iopub.execute_input":"2023-06-24T13:22:38.011436Z","iopub.status.idle":"2023-06-24T13:22:38.028527Z","shell.execute_reply.started":"2023-06-24T13:22:38.011405Z","shell.execute_reply":"2023-06-24T13:22:38.027534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing for Training","metadata":{}},{"cell_type":"markdown","source":"## Setting up the dataset and data loaders for training and validation","metadata":{}},{"cell_type":"code","source":"# Set the data path by concatenating the download path and a directory name\ndownload_path = \"/kaggle/input\"\ndata_path = download_path + '/birdclef-2023/'\n\n# Create a SoundDS object by passing the dataframe and data path as arguments\nmyds = SoundDS(df, data_path)\n\n# Calculate the total number of items in the dataset\nnum_items = len(myds)\n\n# Determine the number of items for training and validation based on an 80:20 split ratio\nnum_train = round(num_items * 0.8)\nnum_val = num_items - num_train\n\n# Split the dataset into training and validation sets using random_split function from PyTorch\ntrain_ds, val_ds = random_split(myds, [num_train, num_val])\n\n# Create a data loader for the training set, with a batch size of 16 and shuffling the data\ntrain_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n\n# Create a data loader for the validation set, with a batch size of 16 and no shuffling\nval_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:22:38.031476Z","iopub.execute_input":"2023-06-24T13:22:38.031754Z","iopub.status.idle":"2023-06-24T13:22:38.047106Z","shell.execute_reply.started":"2023-06-24T13:22:38.031730Z","shell.execute_reply":"2023-06-24T13:22:38.046221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the dataset to the training dataset\ndataset = train_ds\n\n# Iterate through the dataset\nfor i in range(10):\n    # Get the i-th sample from the dataset\n    sample = dataset[i]\n\n    # Extract the input (audio data) and target (label) from the sample\n    input, target = sample\n\n    # Do something with the input and target\n    print(f'Sample {i}: Input shape: {input.shape}, Target: {target}')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:22:38.048588Z","iopub.execute_input":"2023-06-24T13:22:38.048909Z","iopub.status.idle":"2023-06-24T13:22:38.744116Z","shell.execute_reply.started":"2023-06-24T13:22:38.048879Z","shell.execute_reply":"2023-06-24T13:22:38.743094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Initialization and Device Selection","metadata":{}},{"cell_type":"code","source":"# Create an instance of the AudioClassifier model\naudioModel = AudioClassifier()\n\n# Check if CUDA is available and set the device accordingly\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the audioModel to the selected device\naudioModel = audioModel.to(device)\n\n# Get the device of the first parameter of the audioModel\nmodel_device = next(audioModel.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:22:38.745457Z","iopub.execute_input":"2023-06-24T13:22:38.745886Z","iopub.status.idle":"2023-06-24T13:22:41.254344Z","shell.execute_reply.started":"2023-06-24T13:22:38.745853Z","shell.execute_reply":"2023-06-24T13:22:41.252781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model","metadata":{}},{"cell_type":"code","source":"import gc\n# ----------------------------\n# Training Loop\n# ----------------------------\ndef training(model, train_dl, num_epochs):\n    ''' Trains a neural network model using the provided training data loader for the specified number of epochs. Includes Loss Function, Optimizer and Scheduler\n    '''\n  # Loss Function, Optimizer and Scheduler\n  criterion = nn.CrossEntropyLoss()\n  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n                                                steps_per_epoch=int(len(train_dl)),\n                                                epochs=num_epochs,\n                                                anneal_strategy='linear')\n\n  # Repeat for each epoch\n  for epoch in range(num_epochs):\n    running_loss = 0.0\n    correct_prediction = 0\n    total_prediction = 0\n\n    # Repeat for each batch in the training set\n    for i, data in enumerate(train_dl):\n        # Get the input features and target labels, and put them on the GPU\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # Normalize the inputs\n        inputs_m, inputs_s = inputs.mean(), inputs.std()\n        inputs = (inputs - inputs_m) / inputs_s\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        # Keep stats for Loss and Accuracy\n        running_loss += loss.item()\n\n        # Get the predicted class with the highest score\n        _, prediction = torch.max(outputs,1)\n        # Count of predictions that matched the target label\n        correct_prediction += (prediction == labels).sum().item()\n        total_prediction += prediction.shape[0]\n\n        if i % 10 == 0:    # print every 10 mini-batches\n           print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n    \n    # Print stats at the end of the epoch\n    num_batches = len(train_dl)\n    avg_loss = running_loss / num_batches\n    acc = correct_prediction/total_prediction\n    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n\n  print('Finished Training')\n# Train the model\nnum_epochs=2   # Just for demo, adjust this higher.\ntraining(audioModel, train_dl, num_epochs)\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-06-24T13:22:41.261801Z","iopub.execute_input":"2023-06-24T13:22:41.262229Z","iopub.status.idle":"2023-06-24T14:08:03.138916Z","shell.execute_reply.started":"2023-06-24T13:22:41.262189Z","shell.execute_reply":"2023-06-24T14:08:03.137969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions","metadata":{}},{"cell_type":"markdown","source":"## Example Predictions:","metadata":{}},{"cell_type":"code","source":"import librosa\n# Load the audio file using librosa and obtain the audio data and sample rate\naudio0, sample_rate = librosa.load(\"/kaggle/input/birdclef-2023/train_audio/abethr1/XC128013.ogg\")\n\n# Ensure that the sample rate of the audio matches the desired sample rate\nsample_rate, wav_data = ensure_sample_rate(audio0, sample_rate)\n\n# Plot the waveform\nplt.figure(figsize=(10, 4))\nplt.plot(wav_data)\nplt.title('Waveform')\nplt.xlabel('Time')\nplt.ylabel('Amplitude')\nplt.show()\n# Frame the audio data\nfixed_tm = frame_audio(wav_data)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:08:03.140553Z","iopub.execute_input":"2023-06-24T14:08:03.140938Z","iopub.status.idle":"2023-06-24T14:08:27.990269Z","shell.execute_reply.started":"2023-06-24T14:08:03.140903Z","shell.execute_reply":"2023-06-24T14:08:27.989053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def audio_transform(aud, sr, channel, duration, shift_pct):\n    '''Applies the prepared audio transformations to the input audio.\n\n    Args:\n        aud (ndarray): Input audio waveform.\n        sr (int): Sample rate of the audio.\n        channel (int): Target channel of the audio.\n        duration (float): Desired duration of the audio in seconds.\n        shift_pct (float): Percentage of time shift to apply to the audio.\n\n    Returns:\n        ndarray: Augmented spectrogram of the transformed audio.\n'''  \n    # Resample the audio to a desired sample rate\n    reaud = AudioUtil.resample(aud, sr)\n    \n    # Rechannel the audio to a specific channel\n    rechan = AudioUtil.rechannel(reaud, channel)\n    \n    # Pad or truncate the audio to a specified duration\n    dur_aud = AudioUtil.pad_trunc(rechan, duration)\n    \n    # Apply a time shift to the audio\n    shift_aud = AudioUtil.time_shift(dur_aud, shift_pct)\n    \n    # Compute the spectrogram of the shifted audio\n    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n    \n    # Apply spectrogram augmentation, such as frequency and time masking\n    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n\n    return aug_sgram","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:08:27.996941Z","iopub.execute_input":"2023-06-24T14:08:27.997254Z","iopub.status.idle":"2023-06-24T14:08:28.003773Z","shell.execute_reply.started":"2023-06-24T14:08:27.997226Z","shell.execute_reply":"2023-06-24T14:08:28.002838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the audio_transform function with the given arguments\n# The audio_transform function expects a tuple (aud, sr) as the aud parameter, where aud represents the audio data and sr represents the sample rate\n# The audio data is taken from the first element of the fixed_tm array and the sample rate is provided in the sample_rate variable\n# The other arguments are set as follows: 44100 for sr (resampled sample rate), 2 for channel (number of audio channels), 4000 for duration (desired duration in milliseconds), and 0.4 for shift_pct (percentage of time shift)\n# The audio_transform function performs various transformations on the audio data\n# It resamples the audio, rechannels it, pads or truncates it to the desired duration, applies a time shift, computes the spectrogram using mel frequency bands, and applies spectrogram augmentation\n# The resulting augmented spectrogram is assigned to the inputs variable\ninputs = audio_transform((fixed_tm[:1], sample_rate), 44100, 2, 4000, 0.4)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:17:05.467820Z","iopub.execute_input":"2023-06-24T14:17:05.468199Z","iopub.status.idle":"2023-06-24T14:17:05.491787Z","shell.execute_reply.started":"2023-06-24T14:17:05.468168Z","shell.execute_reply":"2023-06-24T14:17:05.490846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the shape of the sliced array fixed_tm[:1]\nfixed_tm_shape = fixed_tm[:1].shape\n\n# Get the shape of the inputs array\ninputs_shape = inputs.shape\n\n# Get the shape of the tensor obtained by unsqueezing the second element of inputs.\n# The unsqueeze() function adds a dimension of size 1 at the specified position (0 in this case)\nunsqueezed_shape = inputs[1].unsqueeze(0).shape\n\n# To match the input shape for the audioModel, add a dimension of size 0 to the inputs tensor using unsqueeze(0).\n# The unsqueezed tensor will have a shape of (1, num_channels, num_frames, num_features)\ninputs_unsqueezed = inputs.unsqueeze(0)\n\n# Move the model to the same device as the input tensor\naudioModel = audioModel.to(inputs_unsqueezed.device)\n\n# Pass the unsqueezed inputs tensor to the audioModel for prediction.\n# The outputs tensor will contain the predicted output from the model\noutputs = audioModel(inputs_unsqueezed)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:17:47.862425Z","iopub.execute_input":"2023-06-24T14:17:47.862788Z","iopub.status.idle":"2023-06-24T14:17:47.891323Z","shell.execute_reply.started":"2023-06-24T14:17:47.862759Z","shell.execute_reply":"2023-06-24T14:17:47.890414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the dimensions of the fixed_tm tensor\nfixed_tm_shape = fixed_tm.shape\nprint(fixed_tm_shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:18:09.147941Z","iopub.execute_input":"2023-06-24T14:18:09.148653Z","iopub.status.idle":"2023-06-24T14:18:09.153785Z","shell.execute_reply.started":"2023-06-24T14:18:09.148619Z","shell.execute_reply":"2023-06-24T14:18:09.152774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open the audio file\naud = AudioUtil.open(\"/kaggle/input/birdclef-2023/train_audio/afghor1/XC156639.ogg\")\n\n# Preprocess the audio data\ninputs = audio_transform(aud, 44100, 2, 4000, 0.4)\n\n# Pass the preprocessed data to the model for prediction\noutputs = audioModel(inputs[0:2].unsqueeze(0))\n\n# Convert the model outputs to probabilities using the sigmoid function\nprobabilities = outputs.sigmoid().cpu().detach().numpy()\n\n# Find the index of the predicted bird species with the highest probability\nargmax = np.argmax(probabilities)\n\n# Print the predicted bird species, its class label, and the probability\nprint(f\"The audio is from the class {classes[argmax]} (element:{argmax} in the label.csv file), with probability of {probabilities[0][argmax]}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:18:12.019943Z","iopub.execute_input":"2023-06-24T14:18:12.020702Z","iopub.status.idle":"2023-06-24T14:18:12.127197Z","shell.execute_reply.started":"2023-06-24T14:18:12.020664Z","shell.execute_reply":"2023-06-24T14:18:12.126143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_for_sample(filename, sample_submission, frame_limit_secs=None):\n    \"\"\"\n    Performs the prediction for a given sample.\n\n    Args:\n        filename (str)\n        sample_submission (pandas.DataFrame): The sample submission DataFrame to update with probabilities.\n        frame_limit_secs (float, optional): The maximum number of seconds to process. Defaults to None.\"\"\"\n    # Extract the file ID from the filename\n    file_id = filename.split(\".ogg\")[0].split(\"/\")[-1]\n    \n    # Load the audio file and ensure the sample rate is correct\n    audio, sample_rate = librosa.load(filename)\n    sample_rate, wav_data = ensure_sample_rate(audio, sample_rate)\n    \n    # Frame the audio data\n    fixed_tm = frame_audio(wav_data)\n    \n    # Transform the input audio data\n    inputs = audio_transform((fixed_tm[:1], sample_rate), 44100, 2, 4000, 0.4)\n    \n    # Set the initial frame number\n    frame = 5\n    \n    # Perform inference on the initial input\n    all_logits = audioModel(inputs.unsqueeze(0)).cpu().detach().numpy()\n    \n    # Iterate over the remaining frames\n    for window in fixed_tm[1:]:\n        # Check if the frame limit has been reached\n        if frame_limit_secs and frame > frame_limit_secs:\n            continue\n        \n        # Transform the input window\n        inputs = audio_transform((window[np.newaxis, :], sample_rate), 44100, 2, 4000, 0.4)\n        \n        # Perform inference on the transformed input\n        logits = audioModel(inputs.unsqueeze(0)).cpu().detach().numpy()\n        \n        # Concatenate the logits with previous results\n        all_logits = np.concatenate([all_logits, logits], axis=0)\n        \n        # Increment the frame number\n        frame += 5\n    \n    # Set the initial frame number\n    frame = 5\n    \n    # Iterate over the logits and compute probabilities\n    all_probabilities = []\n    for frame_logits in all_logits:\n        probabilities = torch.nn.functional.softmax(torch.from_numpy(frame_logits), dim=0).numpy()\n        \n        # Set the appropriate row in the sample submission\n        sample_submission.loc[sample_submission.row_id == file_id + \"_\" + str(frame), competition_classes] = probabilities[competition_class_map]\n        \n        # Increment the frame number\n        frame += 5","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:18:19.138198Z","iopub.execute_input":"2023-06-24T14:18:19.138583Z","iopub.status.idle":"2023-06-24T14:18:19.150473Z","shell.execute_reply.started":"2023-06-24T14:18:19.138553Z","shell.execute_reply":"2023-06-24T14:18:19.149128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"# Get a list of paths for test audio samples\ntest_samples = list(glob.glob(\"/kaggle/input/birdclef-2023/test_soundscapes/*.ogg\"))\n\n# Print the list of test audio sample paths\ntest_samples","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:18:24.771377Z","iopub.execute_input":"2023-06-24T14:18:24.772521Z","iopub.status.idle":"2023-06-24T14:18:24.786916Z","shell.execute_reply.started":"2023-06-24T14:18:24.772482Z","shell.execute_reply":"2023-06-24T14:18:24.785769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the sample submission file into a DataFrame\nsample_sub = pd.read_csv(\"/kaggle/input/birdclef-2023/sample_submission.csv\")\n\n# Convert the columns representing competition classes to float32 data type\nsample_sub[competition_classes] = sample_sub[competition_classes].astype(np.float32)\n\n# Display the first few rows of the DataFrame\nsample_sub.head()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:18:26.964844Z","iopub.execute_input":"2023-06-24T14:18:26.965209Z","iopub.status.idle":"2023-06-24T14:18:27.056587Z","shell.execute_reply.started":"2023-06-24T14:18:26.965179Z","shell.execute_reply":"2023-06-24T14:18:27.055562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the frame limit in seconds based on the number of rows in the sample submission\nframe_limit_secs = 15 if sample_sub.shape[0] == 3 else None\n\n# Iterate over each test sample filename\nfor sample_filename in test_samples:\n    # Call the function to predict for each sample and update the sample submission DataFrame\n    predict_for_sample(sample_filename, sample_sub, frame_limit_secs=15)","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:18:42.488664Z","iopub.execute_input":"2023-06-24T14:18:42.489027Z","iopub.status.idle":"2023-06-24T14:18:44.330708Z","shell.execute_reply.started":"2023-06-24T14:18:42.488998Z","shell.execute_reply":"2023-06-24T14:18:44.329609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:18:46.511936Z","iopub.execute_input":"2023-06-24T14:18:46.512643Z","iopub.status.idle":"2023-06-24T14:18:46.540761Z","shell.execute_reply.started":"2023-06-24T14:18:46.512609Z","shell.execute_reply":"2023-06-24T14:18:46.539761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert our sample submission to csv\nsample_sub.to_csv(\"submission.csv\", index=False)  ","metadata":{"execution":{"iopub.status.busy":"2023-06-24T14:18:52.295594Z","iopub.execute_input":"2023-06-24T14:18:52.296429Z","iopub.status.idle":"2023-06-24T14:18:52.323090Z","shell.execute_reply.started":"2023-06-24T14:18:52.296375Z","shell.execute_reply":"2023-06-24T14:18:52.322222Z"},"trusted":true},"execution_count":null,"outputs":[]}]}